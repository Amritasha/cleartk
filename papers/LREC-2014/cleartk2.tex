\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}

\title{ClearTK 2.0:\\ Lessons learned developing a machine learning framework for UIMA}

\name{Steven Bethard$^1$, Philip Ogren$^2$, Lee Becker$^3$}

\address{%
$^1$University of Alabama at Birmingham, Birmingham, AL, USA, \texttt{bethard@cis.uab.edu}\\
$^2$Oracle America, Broomfield, CO, USA, \texttt{philip@ogren.info}\\
$^3$Hapara Inc., Boulder, CO, USA \texttt{lee@leebecker.com}
}


\abstract{}


\begin{document}

\maketitleabstract

\section{Introduction}
The Unstructured Information Management Architecture (UIMA) framework for developing natural language processing pipelines grew in popularity since it was open-sourced by IBM in 2005.
The framework gained recognition recently for being the underlying architecture of the IBM Watson system that defeated human champions in the game show Jeopardy! \cite{ferrucci_building_2010}.
However, the framework only establishes an architecture for plugging together processing components that agree upon a common type system.
Within the components, there is no support for standard patterns like constructing machine learning classifiers based on sets of features.

The ClearTK framework was introduced to address this gap \cite{ogren-etal:2008:UIMA-LREC,ogren-etal:2009:UIMA-GSCL} by providing:
\begin{itemize}
\item A common interface and wrappers for popular machine learning libraries such as SVMlight, LIBSVM, OpenNLP MaxEnt, and Mallet.
\item A rich feature extraction library that can be used with any of the machine learning classifiers. Under the covers, ClearTK understands each of the native machine learning libraries and translates features into a format appropriate to whatever model is being used.
\item Infrastructure for using and evaluating machine learning classifiers within the UIMA framework.
\end{itemize}

Since its inception in 2008, ClearTK has undergone a large number of changes based on feedback from users and developers.
In this paper, we reflect on key lessons learned over the last 5 years, and how they reflect generally on the development of natural language processing frameworks.

\section{Annotators should look like annotators}

CleartkAnnotator is just a JCasAnnotator

Chunking is just a utility object for use in a JCasAnnotator

Features like TF-IDF are in the Annotator, not in the encoder


\section{Pipelines should look like pipelines}

(Is it that pipelines should look like pipelines or is it that tightly coupled operations should be left together?)

Though most deployed natural language processing systems follow a linear flow (e.g. tokenization, POS-tagging, parsing, \ldots), many tasks require multiple passes through the data.  Common examples of these multi-pass scenarios include model training and evaluation as well as feature normalization.  In developing ClearTK components for more complex flows, we have opted to stucture these abstractions at the granularity of the pipeline.  This in turn makes the programmer's job easier by organizing around an already familiar construct.

\subsection{Case Study: Model Training and Evaluation}
Consider the case of model training and evaluation.  Someone writing an evaluation for a new NLP component is best served by thinking of this in terms of three tasks:

\begin{enumerate}
\item Creation of CollectionReaders to read a subset of the corpus
\item Training a model given a CollectionReader
\item Testing a model given a CollectionReader
\end{enumerate}

This abstraction preserves relationships between tightly coupled operations, while providing the programmer with the freedom to make each task as simple or complex as necessary.  If preprocessing is needed it is simply added to to the training and testing methods.  Even multipass situations like semi-supervised models are permitted with this abstraction.



\textbf{(HELP!!!: I need a way to contrast this with other approaches without dragging everyone into discussion about pipeline providers)}


While it is possible to abstract (HELP: what can I contrast this with?) away the role of the pipeline in these scenarios, we have found that creating interfaces that encourage programmers to think of the subtasks as pipelines



Two examples of require modification and coordination 


New evaluation's train and test methods

Trainable extractors for TF-IDF etc. (not encoders)

By following these principles, on could imagine wrappers to facilitate semi-supervised 
learning 


\section{CollectionReaders should be minimal}

URICollectionReader


\section{Modules should group classes by function}
not organized by type system

cleartk-type-system

cleartk-corpus

cleartk-feature


\section{Type-system-agnostic requires interfaces}

Philip's blog post

weaknesses of OpenNLP approach (e.g., assumes pos is an attribute of token)

ClearNLP work


\section{Users need help past the UIMA overhead}

Write the reader and eval, let the student feature-engineer


\section{Discussion}

\bibliographystyle{lrec2006}
\bibliography{cleartk2}

\end{document}

