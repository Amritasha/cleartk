\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}
\usepackage{listings}

\title{ClearTK 2.0:\\ Lessons learned developing a machine learning framework for UIMA}

\name{Steven Bethard$^1$, Philip Ogren$^2$, Lee Becker$^3$}

\address{%
$^1$University of Alabama at Birmingham, Birmingham, AL, USA, \texttt{bethard@cis.uab.edu}\\
$^2$Oracle America, Broomfield, CO, USA, \texttt{philip@ogren.info}\\
$^3$Hapara Inc., Boulder, CO, USA \texttt{lee@leebecker.com}
}


\abstract{}


\begin{document}

\maketitleabstract

\section{Introduction}
The Unstructured Information Management Architecture (UIMA) framework for developing natural language processing pipelines grew in popularity since it was open-sourced by IBM in 2005.
The framework gained recognition recently for being the underlying architecture of the IBM Watson system that defeated human champions in the game show Jeopardy! \cite{ferrucci_building_2010}.
However, the framework only establishes an architecture for plugging together processing components that agree upon a common type system.
Within the components, there is no support for standard patterns like constructing machine learning classifiers based on sets of features.

The ClearTK framework was introduced to address this gap \cite{ogren-etal:2008:UIMA-LREC,ogren-etal:2009:UIMA-GSCL} by providing:
\begin{itemize}
\item A common interface and wrappers for popular machine learning libraries such as SVMlight, LIBSVM, OpenNLP MaxEnt, and Mallet.
\item A rich feature extraction library that can be used with any of the machine learning classifiers. Under the covers, ClearTK understands each of the native machine learning libraries and translates features into a format appropriate to whatever model is being used.
\item Infrastructure for using and evaluating machine learning classifiers within the UIMA framework.
\end{itemize}

Since its inception in 2008, ClearTK has undergone a large number of changes based on feedback from users and developers.
In this paper, we reflect on key lessons learned over the last 5 years, and how they reflect generally on the development of natural language processing frameworks.

\section{Annotators should look like annotators}

CleartkAnnotator is just a JCasAnnotator

Chunking is just a utility object for use in a JCasAnnotator

Features like TF-IDF are in the Annotator, not in the encoder


\section{Pipelines should look like pipelines}

(Is it that pipelines should look like pipelines or is it that tightly coupled operations should be left together?)

Though most deployed natural language processing systems follow a linear flow (e.g. tokenization, POS-tagging, parsing, \ldots), many tasks require multiple passes through the data.  Common examples of these multi-pass scenarios include model training and evaluation as well as feature normalization.  In developing ClearTK components for more complex flows, we have opted to stucture these abstractions at the granularity of the pipeline.  This in turn makes the programmer's job easier by organizing around an already familiar construct, and reduces the barrier to adoption of new frameworks.

\subsection{Model Training and Evaluation}
Consider the case of model training and evaluation.  Someone writing an evaluation for a new NLP component is best served by thinking of this in terms of three tasks:

\begin{enumerate}
\item Creation of CollectionReaders to read a subset of the corpus
\item Training a model given a CollectionReader
\item Testing a model given a CollectionReader
\end{enumerate}


This abstraction preserves relationships between tightly coupled operations, while providing the programmer with the freedom to make each task as simple or complex as necessary.  If preprocessing is needed it is simply added to to the training and testing methods.  Even multipass situations like semi-supervised models are permitted with this abstraction.  Most importantly, an existing pipeline can be dropped in place into the evaluation framework without need for refactoring or alteration.

\textbf{(HELP!!!: I need a way to contrast this with other approaches without dragging everyone into discussion about pipeline providers)}

% this is probably not necessary
\lstinputlisting[language=Java]{eval.java}

\subsection{Feature Transformation}

Feature normalization is a common prerequisite for many machine learning algorithms.  For information retrieval tasks it is common to scale term counts by inverse document frequency or to transform it into a z-score (zero mean, unit standard deviation).

At its core feature transformation consists of:
\begin{enumerate}
\item Extracting features
\item Identifying features in need of transformation
\item Computing sufficient statistics for transformation
\item Transforming the features
\end{enumerate}

When dealing solely with a dataset, these steps are simply a matter of selecting and transforming column values.  However, in processing and pipeline oriented environments, this is not possible.  One workaround to this limitation involves extracting the statistics independently of the pipeline, and then configuring analysis engines to accept these statistics as parameters for scaling during feature extraction.

It has been our experience that this decoupling is error prone and leads to long-term maintenance issues, especially when dealing with ever changing corpora or evaluation parameters.  Although there is no way to avoid performing feature scaling in two passes, the ClearTK approach makes feature transformation more accessible by making it interoperable with the existing ClearTK extraction, training, and classification operations.

To facilitate this, ClearTK introduces the \emph{TransformableFeatures} and \emph{TrainableFeatureExtractors} and {\emph InstanceDataWriter} classes.  Like all other feature extractors in ClearTK, a TrainableFeaturesExtractor accepts a JCas and an Annotation and returns a list of features.  In practice most TrainableFeatureExtractors delegate extraction to a sub-extractor, and instead of returning standard features it returns a list of TransformableFeatures.  ClearTK includes TrainableFeatureExtractors for common transformations such as z-score, tf*idf, min-max normalization and cosine similarity.

Typically when training a CleartkAnnotator, the extracted feature vectors are written directly to a data file for consumption by the learning algorithm.  When using TrainableFeatureExtractors, the feature vectors are instead serialized using the InstanceDataWriter.  This enables the TrainableFeatureExtractors to iterate over the instances as a collection and compute the necessary statistics for direct transformation.

After training each TrainableFeatureExtractor, the TransformableFeatures within the instances are transformed and then written out to the data file for training of the classifier.
During classification, TrainableFeatureExtractors load a configuration file and directly transform the features for use by the classifier.

\textbf{Struggling to succinctly wrap this up}.  This approach to feature scaling provides a means of wrangling a complex process into the paradigms already employed by users of ClearTK and UIMAFit.

%Two examples of require modification and coordination 
%
%
%New evaluation's train and test methods
%
%Trainable extractors for TF-IDF etc. (not encoders)
%
%By following these principles, on could imagine wrappers to facilitate semi-supervised 
%learning 


\section{CollectionReaders should be minimal}

URICollectionReader


\section{Modules should group classes by function}
not organized by type system

cleartk-type-system

cleartk-corpus

cleartk-feature


\section{Type-system-agnostic requires interfaces}

Philip's blog post

weaknesses of OpenNLP approach (e.g., assumes pos is an attribute of token)

ClearNLP work


\section{Users need help past the UIMA overhead}

Write the reader and eval, let the student feature-engineer


\section{Discussion}

\bibliographystyle{lrec2006}
\bibliography{cleartk2}

\end{document}

