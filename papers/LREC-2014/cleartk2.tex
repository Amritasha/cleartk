\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{microtype}
\usepackage{cleveref}

\newcommand{\code}[1]{\texttt{\small #1}}

\title{ClearTK 2.0: Design Patterns for Machine Learning in UIMA}

\name{Steven Bethard$^1$, Philip Ogren$^2$, Lee Becker$^2$}

\address{%
\texttt{bethard@cis.uab.edu}, \texttt{ogren@colorado.edu}, \texttt{lee.becker@colorado.edu},  \\
$^1$University of Alabama at Birmingham, Birmingham, AL, USA,  \\
$^2$University of Colorado at Boulder, Boulder, CO, USA 
}

\hyphenation{Annotation-Handler Classifier-Annotator Chunker-Feature-Extractor Collection-Reader URI-Collection-Reader}

\abstract{}


\begin{document}

\maketitleabstract

\section{Introduction}
The Unstructured Information Management Architecture (UIMA) framework for developing natural language processing pipelines grew in popularity since it was open-sourced by IBM in 2005.
The framework gained recognition recently for being a core underlying architecture of the IBM Watson system that defeated human champions in the game show Jeopardy! \cite{ferrucci_building_2010}.
However, the framework only establishes an architecture for plugging together processing components and does not directly support constructing machine learning classifiers based on sets of features.

The ClearTK framework was introduced to address this gap \cite{ogren-etal:2008:UIMA-LREC,ogren-etal:2009:UIMA-GSCL} by providing:
\begin{itemize}
\item A common interface and wrappers for popular machine learning libraries such as SVMlight, LIBSVM, LIBLINEAR, OpenNLP MaxEnt, and Mallet.
\item A rich feature extraction library that can be used with any of the machine learning classifiers. Under the covers, ClearTK understands each of the native machine learning libraries and translates features into a format appropriate to whatever model is being used.
\item Infrastructure for using and evaluating machine learning classifiers within the UIMA framework.
\end{itemize}

Since its inception in 2008, ClearTK has undergone a large number of changes based on feedback from users and developers.
In this paper, we reflect on key lessons learned over the last 5 years, and how they reflect generally on the development of natural language processing frameworks.

\section{Annotators should be conceptually simple}
\label{sec:annotators}
At the core of UIMA is the idea of an annotator, which provides a \code{process()} method that inspects the document (i.e. the CAS), performs some analysis, and adds annotations representing the result of that analysis.  This is a construct familiar even to novice UIMA users, and there are analogs in other frameworks, e.g. Stanford CoreNLP's \code{CoreMap}\footnote{http://nlp.stanford.edu/software/corenlp.shtml} and GATE's \code{Document}\footnote{http://gate.ac.uk/ie/}.  Our experience suggests that annotators' process methods should orchestrate the core analysis code in ways that are as straightforward and intuitive as possible.  ClearTK in previous iterations, has suffered from abstracting out analysis code away from the process method in ways that made the code difficult to understand.  

In the first versions of ClearTK, developers were required to write two separate annotators: one for writing training data and another for classification.
In practice, the bulk of the code in both annotators was the same feature extraction steps.
So this shared functionality was abstracted to remove this redundancy of code.
The first version of this abstraction used a call-back approach where users only had to implement the feature extraction code.
However, the call-back style was unlike the the traditional UIMA \code{process()} method, and users found this unintuitive.
Thus, we therefore redesigned this abstraction, introducing \code{ClassifierAnnotator}, which allows users to write their feature extraction code in the standard \code{process()} method, but requires them to handle two cases: training mode and classification mode.
While this required a bit more orchestration by a single annotator, it was a conceptually simpler approach that was much easier to learn.  

% this is similar to the above; could be cut if we need space
We learned a similar lesson in designing an API for Begin-Inside-Outside (BIO) style chunking classification.
Our original chunker framework defined a chunker as a subtype of \code{ClassifierAnnotator} that abstracted away different aspects of chunking, such as converting token-based labels to chunks, converting chunks to token-based labels, and performing token-level feature extraction.
How and when these activities were performed were based on classes passed to the chunker and dynamically loaded at instantiation time.
In ClearTK 1.2 we replaced this architecture with a set of utility objects for converting chunk labels into token labels (and vice versa) that could be used directly in a standard annotator with a standard \code{process(JCas)} method without the developer needing to implement or tie together specialized implementations of interfaces.  

% Features like TF-IDF are in the Annotator, not in the encoder
% I decided to skip this; is seemed too complicated to explain 

\section{Pipelines should look like pipelines}
Once a user has developed a number of annotators, they typically string them together in a \emph{pipeline}, indicating the sequence these annotators should be run on a new piece of text.
In ClearTK, users develop a variety of different pipelines: pipelines for training classifiers, pipelines for applying trained classifiers to make predictions, pipelines for testing classifier predictions against a gold standard, etc.
Our experience suggests that pipeline-based code should be structured to make it easy to quickly understand what annotators are running in what order.

Consider the case of model training and evaluation.
ClearTK's first abstraction for this separated out:
\begin{itemize}
\item The reader that loaded the training and testing data
\item The preprocessing portion of a pipeline
\item The classifier training portion of a pipeline
\item The classifier prediction portion of a pipeline
\item The evaluation portion of a pipeline
\end{itemize}
These items are easily separable and splitting them reduced code duplication. (For example, the processing portion of the pipeline would be identical for training and testing).
However, because each of these items was implemented in a different class or method, it was often difficult for a reader to understand the big picture of what exactly was running in each pipeline.
In ClearTK 1.2, we simplified this abstraction, resulting in the following division of labor for evaluation:
\begin{enumerate}
\item Reading a subset of data with a \code{CollectionReader}
\item Training a model given a \code{CollectionReader}
\item Testing a model given a \code{CollectionReader}
\end{enumerate}
This means that there is potentially some duplication, e.g. if training and testing used the same preprocessing.
But a user can now easily examine the complete training or testing pipeline by looking at a single method.

As another example of structuring code for pipelines, consider the case of feature transformations, e.g. normalizing feature values to zero mean and unit standard deviation, or scaling term counts by inverse document frequency.
In early versions of ClearTK, these kinds of transformations required a specialized pipeline to be run separately before the real pipeline to collect the sufficient statistics.
This was confusing to users because (1) two pipelines were required for what was conceptually a single pipeline and (2) feature transformations conceptually happen after training data is written, not before.
ClearTK 1.2 introduced \code{TrainableFeatureExtractor}s which instead work like:
\begin{itemize}
\item Run the original pipeline for writing training data. The \code{TrainableFeatureExtractor} will flag features that need additional post-processing.
\item End the pipeline with an \code{InstanceDataWriter} that serializes the features for re-use.
\item Invoke the \code{TrainableFeaturesExtractor}'s \code{train()} method on the serialized features to store sufficient statistics.
\item Use the original data writer on the transformed features to write out the training data for the classifier of choice.
\end{itemize}
We found that this approach was a better match for the conceptual expectations of our users.


\section{CollectionReaders should be minimal}
In UIMA, a \code{CollectionReader} is the connection between the source (file, URL, etc.) and the UIMA document (\code{JCas}) object.
Early versions of ClearTK used the \code{CollectionReader} mechanism to both read in the text and import various annotation formats (TreeBank, PropBank, etc.).

However, as ClearTK developed support for importing more annotation formats, it became clear that this approach was problematic.
UIMA allows only a single \code{CollectionReader} at the beginning of each pipeline, so you cannot, for example, have both a \code{CollectionReader} for TreeBank and one for TimeML in the same pipeline, even if both layers of annotation exist for your document.
The solution to this problem is to view these TreeBank and TimeML readers not as \code{CollectionReader}s, but as regular UIMA annotators whose \code{process(JCas)} method looks at some external resource (e.g. a \code{.mrg} or a \code{.tml} file) and converts that into annotations to be added to the \code{JCas}.

ClearTK now recommends only one \code{CollectionReader}, \code{URICollectionReader}, which does nothing more than create a \code{JCas} containing the source's Uniform Resource Identifier (URI).
Reading the text or annotations on top of the text must then be performed by subsequent annotators.
This approach has several advantages, including that the pipelines become more parallelizable (which UIMA-AS can take advantage of), and that users can leverage their existing familiarity with UIMA annotators when writing readers.

\section{Modules should match natural subsets}
ClearTK provides many different types of utilities (machine learning wrappers, readers for various corpora, UIMA wrappers for non-UIMA components like MaltParser or Stanford CoreNLP, etc.) and so it has been necessary to split ClearTK up into a small number of modules to allow users to depend on only those parts of ClearTK that they need.
In early versions of ClearTK, we structured these based on the types of annotations being processed, e.g. code for reading PennTreebank trees was put into the same module as our wrapper for OpenNLP's parser.
The idea was that if you were working on, say, parsing, you would want access to all the different parsing algorithms.
However, we found that this approach didn't scale.
For example, very few users would want to include all of OpenNLP, MaltParser, BerkeleyParser, Stanford CoreNLP, etc. just to read trees from a PennTreebank file.

ClearTK 1.2 restructured the modules to match the natural subsets of ClearTK that users might want:
\begin{itemize}
\item The machine learning libraries and generic feature extractors that do not require a UIMA type system
\item ClearTK's version of a UIMA type system for NLP
\item Feature extractors based on the ClearTK type system (e.g. paths through constituency trees)
\item Readers for various corpora, based on the ClearTK type system
\item Wrappers for non-UIMA components, based on the ClearTK type system
\end{itemize}
We have found that this structure better matches the conceptual dependencies of ClearTK, and better enables ClearTK users to get just the parts that they want.


\section{Decoupling infrastructure from meta-data}

The output of the analytics of a text processing system is invariably determined by meta-data that defines the structure of the resulting analysis.  In UIMA parlance, this meta-data is defined by a type system which defines annotation types such as tokens or named entities and their features such as part-of-speech tags or unique identifiers.  Type systems, like all meta data definitions, vary widely across different UIMA-based systems due to the widely varying requirements imposed by different domains and use cases.  Not surprisingly, it is a difficult challenge to produce meta-data standards that are used widely across different projects and institutions.  With this in mind, we have been careful to decouple ClearTK from any specific type system.  In fact, the core framework that ClearTK provides does not require (or provide) any specific type system.  This is one of the key strengths of ClearTK: all of the machinery for creating classifier-based annotators including feature extraction, feature normalization, chunking, training, classification, etc. is completely type system independent.  This makes ClearTK a powerful framework to employ in a very wide vareity of scenarios.  A case in point is a feature extractor called \code{CleartkExtractor}.  This feature extractor allows one to create features such as ``get the text of the token two tokens before the focus annotation'' or ``get the part-of-speech tag of the token three tokens after the focus annotation''.  These and many more related feature extraction scenarios are all accomplished by instantiating a CleartkExtractor with the types specific to a CleartkAnnotator's type system.  This eliminates the need to write many specialized feature extractors for different type systems.  This is true for all of the feature extractors provided the core ClearTK project including those that generate bag-of-words features and TF-IDF weighted features as well as the code that supports BIO-style chunking.  Thus, the only code that needs to know about a specific type system is the code written by a user of ClearTK who is implementing analysis engines with the framework.  Some parts of ClearTK do depend on specific type systems such as the state-of-the-art temporal event extraction system that is provided as a sub-project of ClearTK.  However, we have been careful to keep infrastructure code that provides the framework for creating classifier-based annotators decoupled from any type system.  




ClearNLP work


\section{Users need help past the UIMA overhead}
After the many improvements to ClearTK interfaces and usability over the years, we've now reached a point where much of the overhead of learning ClearTK is actually the overhead of learning UIMA.
To understand the UIMA framework, you need to understand not just how to write an annotator with a \code{process(JCas)} method -- which is what's really at the heart of the framework -- but also how to:
\begin{itemize}
\item Declare a type system that describes the annotations you want your annotator to create
\item Configure your build system to generate Java classes from the type system
\item Create code to read your training data into \code{JCas} objects
\item Declare (using XML files or Java annotations) any parameters needed to initialize your annotator
\item Create an \code{AnalysisEngine} object from your annotator and the initialization parameters
\item \ldots
\end{itemize}
These tasks are fairly easy for a UIMA expert, but are often challenging and overwhelming for a new UIMA user.
Thus, to get new potential users of ClearTK up to speed, we've found that it can be quite helpful to have a UIMA expert put the above items together.
Then, the new users can focus on the core problems that ClearTK is designed for: extracting features and using the classifier in the \code{process(JCas)} method of the annotator.
We applied exactly this approach with new users of ClearTK, and successfully developed both a student response analysis system for SemEval-2013 \cite{okoye-bethard-sumner:2013:SemEval-2013}, and a relation extraction system for Apache cTAKES \cite{dligach2013discovering}.

This paper should mention the state-of-the art temporal extraction system that has been built with ClearTK somewhere.  Also, did we ever get any feedback from the survey?

\section{Discussion}

\bibliographystyle{lrec2006}
\bibliography{cleartk2}

\end{document}

