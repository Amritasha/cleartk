\documentclass[10pt, a4paper]{article}
\usepackage{lrec2006}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{microtype}
\usepackage{cleveref}

\newcommand{\code}[1]{\texttt{\small #1}}

\title{ClearTK 2.0: Design Patterns for Machine Learning in UIMA}

\name{Steven Bethard$^1$, Philip Ogren$^2$, Lee Becker$^2$}

\address{%
\texttt{bethard@cis.uab.edu}, \texttt{ogren@colorado.edu}, \texttt{lee.becker@colorado.edu},  \\
$^1$University of Alabama at Birmingham, Birmingham, AL, USA,  \\
$^2$University of Colorado at Boulder, Boulder, CO, USA 
}

\hyphenation{Annotation-Handler Classifier-Annotator Chunker-Feature-Extractor Collection-Reader URI-Collection-Reader}

\abstract{}


\begin{document}

\maketitleabstract

\section{Introduction}
The Unstructured Information Management Architecture (UIMA) framework for developing natural language processing pipelines has grown in popularity since it was open-sourced by IBM in 2005.  More recently, UIMA has gained recognition as the underlying architecture of the IBM Watson system that defeated human champions in the game show Jeopardy! \cite{ferrucci_building_2010}.
However, the framework only establishes an architecture for connecting NLP components and does not directly support constructing machine learning classifiers based on sets of features.

The ClearTK framework was introduced to address this gap \cite{ogren-etal:2008:UIMA-LREC,ogren-etal:2009:UIMA-GSCL} by providing:
\begin{itemize}
\item A common interface and wrappers for popular machine learning libraries such as SVMlight, LIBSVM, LIBLINEAR, OpenNLP MaxEnt, and Mallet.
\item A rich feature extraction library that can be used with any of the machine learning classifiers. Under the covers, ClearTK understands each of the native machine learning libraries and translates features into a format appropriate to whatever model is being used.
\item Infrastructure for using and evaluating machine learning classifiers within the UIMA framework.
\end{itemize}

Since its inception in 2008, ClearTK has been adopted by multiple developers worldwide in both academia and industry.  It is now starred by 78 users in Google Code, and in the past year ClearTK has been downloaded over 1700 times.  This growing user and developer base has provided a wealth of feedback that has led to a large number of changes.  In this paper, we reflect on key lessons learned over the last 5 years, and how they reflect generally on the design of natural language processing frameworks.

\section{Annotators should be conceptually simple}
\label{sec:annotators}
A core aspect of UIMA is the Annotator, which provides a \code{process()} method that inspects the document, performs analyses, and stores resulting annotations.  This is a familiar construct, even to novice UIMA users, as there are analogs in other frameworks, e.g. Stanford CoreNLP's \code{CoreMap}\footnote{http://nlp.stanford.edu/software/corenlp.shtml} and GATE's \code{Document}\footnote{http://gate.ac.uk/ie/}.  Our experience suggests that annotators' process methods should orchestrate the core analysis code in ways that are as straightforward and intuitive as possible.  ClearTK in previous iterations, has suffered from abstracting out analysis code away from the process method in ways that made the code difficult to understand.  

In the first versions of ClearTK, developers were required to write two separate annotators: one for writing training data and another for classification.
In practice, the bulk of the code in both annotators shared the same feature extraction code.  Consequently, this overlapping functionality was structured to remove this redundancy.  The first version of this abstraction used a call-back approach where users only had to implement the feature extraction code.  However, users found the call-back style unintuitive as it flouted the UIMA \code{process()} conventions.  
Thus we redesigned this abstraction into \code{ClassifierAnnotator}, which allows users to write their feature extraction code in the standard \code{process()} method, but requires them to handle two cases: training mode and classification mode.  Though this change requires additional coordinatino with a single annotator, it is conceptually simpler to learn.

% this is similar to the above; could be cut if we need space
We learned a similar lesson in designing an API for Begin-Inside-Outside (BIO) style chunking classification.
Our original chunker framework defined a chunker as a subtype of \code{ClassifierAnnotator} that abstracted away different aspects of chunking, such as converting token-based labels to chunks, converting chunks to token-based labels, and performing token-level feature extraction.
How and when these activities were performed were based on classes passed to the chunker and dynamically loaded at instantiation time.
In ClearTK 1.2 we replaced this architecture with a set of utility objects for converting chunk labels into token labels (and vice versa) that could be used directly in a standard annotator with a standard \code{process()} method without the developer needing to implement or tie together specialized implementations of interfaces.  

% Features like TF-IDF are in the Annotator, not in the encoder
% I decided to skip this; is seemed too complicated to explain 

\section{Pipelines should look like pipelines}
Once a user has developed a number of annotators, they typically string them together in a \emph{pipeline}, indicating the sequence these annotators should be run on a new piece of text.
In ClearTK, users develop a variety of different pipelines: pipelines for training classifiers, pipelines for applying trained classifiers to make predictions, pipelines for testing classifier predictions against a gold standard, etc.
Our experience suggests that pipeline-based code should be structured to make it easy to quickly understand what annotators are running in what order.

Consider the case of model training and evaluation.
ClearTK's first abstraction for this separated out:
\begin{itemize}
\item The reader that loaded the training and testing data
\item The preprocessing portion of a pipeline
\item The classifier training portion of a pipeline
\item The classifier prediction portion of a pipeline
\item The evaluation portion of a pipeline
\end{itemize}
These items are easily separable and splitting them reduced code duplication. (For example, the processing portion of the pipeline would be identical for training and testing).
However, because each of these items was implemented in a different class or method, it was often difficult for a reader to understand the big picture of what exactly was running in each pipeline.
In ClearTK 1.2, we simplified this abstraction, resulting in the following division of labor for evaluation:
\begin{enumerate}
\item Reading a subset of data with a \code{CollectionReader}
\item Training a model given a \code{CollectionReader}
\item Testing a model given a \code{CollectionReader}
\end{enumerate}
This means that there is potentially some duplication, e.g. if training and testing used the same preprocessing.
But a user can now easily examine the complete training or testing pipeline by looking at a single method.

As another example of structuring code for pipelines, consider the case of feature transformations, e.g. normalizing feature values to zero mean and unit standard deviation, or scaling term counts by inverse document frequency.
In early versions of ClearTK, these kinds of transformations required a specialized pipeline to be run separately before the real pipeline to collect the sufficient statistics.
This was confusing to users because (1) two pipelines were required for what was conceptually a single pipeline and (2) feature transformations conceptually happen after training data is written, not before.
ClearTK 1.2 introduced \code{TrainableFeatureExtractor}s which instead work like:
\begin{itemize}
\item Run the original pipeline for writing training data. The \code{TrainableFeatureExtractor} will flag features that need additional post-processing.
\item End the pipeline with an \code{InstanceDataWriter} that serializes the features for re-use.
\item Invoke the \code{TrainableFeaturesExtractor}'s \code{train()} method on the serialized features to store sufficient statistics.
\item Use the original data writer on the transformed features to write out the training data for the classifier of choice.
\end{itemize}
We found that this approach aligned better with the conceptual expectations of our users.


\section{Collection readers should be minimal}
In UIMA, a \code{CollectionReader} is the connection between the source (file, URL, etc.) and the UIMA document (\code{JCas}) object.
Early versions of ClearTK used the \code{CollectionReader} mechanism to both read in the text and import various annotation formats (TreeBank, PropBank, etc.).

However, as ClearTK developed support for importing more annotation formats, it became clear that this approach was problematic.
UIMA allows only a single \code{CollectionReader} at the beginning of each pipeline, so you cannot, for example, have both a \code{CollectionReader} for TreeBank and one for TimeML in the same pipeline, even if both layers of annotation exist for your document.
The solution to this problem is to view these TreeBank and TimeML readers not as \code{CollectionReader}s, but as regular UIMA annotators whose \code{process(JCas)} method looks at some external resource (e.g. a \code{.mrg} or a \code{.tml} file) and converts that into annotations to be added to the \code{JCas}.

ClearTK now recommends only one \code{CollectionReader}, \code{URICollectionReader}, which does nothing more than create a \code{JCas} containing the source's Uniform Resource Identifier (URI).
Reading the text or annotations on top of the text must then be performed by subsequent annotators.
This approach to developing readers has several advantages, including more parallizable pipelines (which UIMA-AS can take advantage of) and added accessibility by leveraging users' existing familiarity with UIMA annotators.


\section{Code should be type system agnostic}
All UIMA annotators must declare a type system, which defines the annotations and attributes that an annotator may add to documents.
Due to varying requirements imposed by different domains and use cases, there is not yet a generally agreed upon NLP type system for UIMA, and thus many UIMA annotators cannot be combined easily.
In ClearTK, we have always been careful to decouple the machine learning framework from the type system. All of the machinery for creating classifier-based annotators including feature extraction, feature normalization, chunking, training, classification, etc. is completely type system independent.

%A case in point is a feature extractor called \code{CleartkExtractor}.  This feature extractor allows one to create features such as ``get the text of the token two tokens before the focus annotation'' or ``get the part-of-speech tag of the token three tokens after the focus annotation''.  These and many more related feature extraction scenarios are all accomplished by instantiating a CleartkExtractor with the types specific to a CleartkAnnotator's type system.  This eliminates the need to write many specialized feature extractors for different type systems.  This is true for all of the feature extractors provided the core ClearTK project including those that generate bag-of-words features and TF-IDF weighted features as well as the code that supports BIO-style chunking.  Thus, the only code that needs to know about a specific type system is the code written by a user of ClearTK who is implementing analysis engines with the framework.

However, other parts of ClearTK do depend on a specific type system, e.g. for reading different annotations from corpora, for wrapping the output produced by non-UIMA annotators, and for constructing state-of-the-art systems like ClearTK-TimeML \cite{bethard:2013:SemEval-2013}.
It is quite difficult to write a truly type-system agnostic UIMA annotator.
For example, the OpenNLP UIMA annotators are intended to be type system agnostic, but in fact make type-system specific assumptions, like that the part-of-speech is represented as a string-valued attribute of a token annotation.
To avoid this level of specific type system dependence, we have found it to be necessary to define interfaces for the various operations on tokens, sentences, parses, etc.
Such an approach has been implemented for the ClearTK wrappers for ClearNLP, and we hope to extend this to other areas as well.


\section{Modules should match natural subsets}
ClearTK provides many different types of utilities (machine learning wrappers, readers for various corpora, UIMA wrappers for non-UIMA components like MaltParser or Stanford CoreNLP, etc.) and so it has been necessary to split ClearTK up into a small number of modules to allow users to depend on only those parts of ClearTK that they need.
In early versions of ClearTK, we structured these based on the types of annotations being processed, e.g. code for reading PennTreebank trees was put into the same module as our wrapper for OpenNLP's parser.
The idea was that if you were working on, say, parsing, you would want access to all the different parsing algorithms.
However, we found that this approach didn't scale.
For example, very few users would want to include all of OpenNLP, MaltParser, BerkeleyParser, Stanford CoreNLP, etc. just to read trees from a PennTreebank file.

ClearTK 1.2 restructured the modules to match the natural subsets of ClearTK that users might want:
\begin{itemize}
\item The machine learning libraries and generic feature extractors that do not require a UIMA type system
\item ClearTK's version of a UIMA type system for NLP
\item Feature extractors based on the ClearTK type system (e.g. paths through constituency trees)
\item Readers for various corpora, based on the ClearTK type system
\item Wrappers for non-UIMA components, based on the ClearTK type system
\end{itemize}
We have found that this structure better matches the conceptual dependencies of ClearTK, and better enables ClearTK users to use only the parts they want.


\section{Users need help past the UIMA overhead}
After the many improvements to ClearTK interfaces and usability over the years, we've now reached a point where much of the overhead of learning ClearTK is actually the overhead of learning UIMA.
To understand the UIMA framework, you need to understand not just how to write an annotator with a \code{process(JCas)} method -- which is what is really at the heart of the framework -- but also how to:
\begin{itemize}
\item Declare a type system that describes the annotations you want your annotator to create
\item Configure your build system to generate Java classes from the type system
\item Create code to read your training data into \code{JCas} objects
\item Declare (using XML files or Java annotations) any parameters needed to initialize your annotator
\item Create an \code{AnalysisEngine} object from your annotator and the initialization parameters
\item \ldots
\end{itemize}
These tasks are fairly easy for a UIMA expert, but are often challenging and overwhelming for a new UIMA user.
Thus, to get new potential users of ClearTK up to speed, we have found it helpful to have a UIMA expert put the above items together.
Then, the new users can focus on the core problems that ClearTK is designed for: extracting features and using the classifier in the \code{process(JCas)} method of the annotator.
We applied exactly this approach with new users of ClearTK, and successfully developed both a student response analysis system for SemEval-2013 \cite{okoye-bethard-sumner:2013:SemEval-2013}, and a relation extraction system for Apache cTAKES \cite{dligach2013discovering}.

\section{Conclusion}
The development of the ClearTK framework has revealed a number of key design patterns for NLP frameworks that can help new users to more quickly understand and adopt a framework.
At their core, these patterns suggest aiming for intuitive interfaces that leverage the existing knowledge of users, and trying to minimize the number of conceptual dependencies between the various parts of the framework.

\bibliographystyle{lrec2006}
\bibliography{cleartk2}

\end{document}

